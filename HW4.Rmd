---
title: "Linear Model & Regularization"
output: html_notebook
---

1. In this exercise, we will predict the number of applications received using the other variables in the College data set. 

(a) Split the data set into a training set and a test set.

```{r}
library(ISLR)
attach(College)

head(College)
```
```{r}
set.seed(1)


train = sample(dim(College)[1], size = 0.50*dim(College)[1], replace=FALSE)	# set 50% of data set as training set
test = -train 				# set remaining 50% of data set as tesr set

training.set = College[train, ]		# split training set
test.set= College[test, ]		# split test set

dim(College) # entire data set
dim(training.set) # training set
dim(test.set) # Validation set
```

(b) Fit a linear model using least-squares on the training set, and report the test error obtained. 

```{r}
fit.lm = lm(Apps~., data=training.set) # fit linear model on training set using least squares 
pred.lm = predict(fit.lm, test.set) # make prediction on test set
error.lm = mean((test.set$Apps - pred.lm)^2) # calculate test error using Mean Squared Error
error.lm
rmse.lm = sqrt(error.lm) # find RMSE of predictions
rmse.lm

```

(c) Fit a ridge regression model on the training set, with 位 chosen by cross-validation. Report the test error obtained. 

```{r}
library(glmnet) # package does not use model formula language

x.train = model.matrix(Apps~., data=training.set) # set up x.train (predictors)
y.train = training.set$Apps # set up y.train (response)

x.test = model.matrix(Apps~., data=test.set) # set up x (predictors)
y.test = test.set$Apps
  
# model selection using ridge regression
fit.ridge = glmnet(x.train, y.train, alpha=0) # fit ridge regression
plot(fit.ridge, xvar="lambda", label=TRUE) # plot coefficients of the predictors; recall that the ridge regressions model is penalized by the sum of squares of the coefficients multiplied by lambda

cv.ridge = cv.glmnet(x.train, y.train, alpha=0) # "cv.glmnet" is a built-in function used to perform cross-validation, with k=10 as default
cv.ridge
bestlambda = cv.ridge$lambda.min # select minimum lambda model

# prediction using ridge regression 
pred.ridge = predict(fit.ridge, s=bestlambda, newx=x.test)

error.ridge = mean((y.test - pred.ridge)^2)
error.ridge
rmse.ridge = sqrt(error.ridge)  # find RMSE of ridge regression model
rmse.ridge
```

(d) Fit a lasso model on the training set, with 位 chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.

```{r}
fit.lasso = glmnet(x.train, y.train, alpha=1) # fit lasso model
plot(fit.lasso, xvar="lambda", label=TRUE) # plot coefficients of the predictors; recall that the ridge regressions model is penalized by the sum of squares of the coefficients multiplied by lambda

cv.lasso = cv.glmnet(x.train, y.train, alpha=1) # "cv.glmnet" is a built-in function used to perform cross-validation, with k=10 as default
cv.lasso
bestlambda = cv.lasso$lambda.min # choose minimum lambda model

pred.lasso = predict(fit.lasso, s=bestlambda, newx=x.test)

error.lasso = mean((y.test - pred.ridge)^2)
error.lasso
rmse.lasso = sqrt(error.lasso)  # find RMSE of lasso model
rmse.lasso

coef(fit.lasso, s=bestlambda) # show the number of non-zero coefficient estimates
predict(fit.lasso, s = bestlambda, type = "coefficients")
```

(g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

Answer:
======
The results obtained showed that the Ridge regression model and the Lasso model were a better fit for the College data set. This means we have a better chance of predicting the number of college applications received more accurately using either model. Also, while the test errors of both models are similar, the test error of the linear model is substantially different.


2. We will now try to predict per capita crime rate in the Boston data set. 
(a) Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider. 


Split Data Set into Training and Test set
-----------------------------------------

```{r}
library(MASS) # load library
attach(Boston) # makes College variables available by name

set.seed(15)

train = sample(dim(Boston)[1], size = 0.50*dim(Boston)[1], replace=FALSE)	# set 50% of data set as training set
test = -train 				# set remaining 50% of data set as test set

training.set = Boston[train, ]		# split training set
test.set= Boston[test, ]		# split test set

dim(Boston) # entire data set
dim(training.set) # training set
dim(test.set) # Validation set

```

Best Subset Selection
---------------------
```{r}
library(leaps)                                     # load library
set.seed(15)                                       # set random seed
folds=sample(rep(1:10, length=nrow(Boston)))       # create 10 folds
table(folds)                                       # show fold
cv.errors = matrix(NA, 10, 13) # create a matrix store the cross validation errors for the 13 subsets


predict.regsubsets=function(object, newdata, id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form, newdata)
  coefi=coef(object, id=id)
  mat[,names(coefi)] %*% coefi
}                                                 # write method to predict for regsubset


for(k in 1:10){0
  best.fit=regsubsets(crim~., data=Boston[folds!=k, ], nvmax=13, method="forward")
  for(i in 1:13){
    pred=predict(best.fit, Boston[folds==k, ], id=i)
    cv.errors[k,i]=mean( (Boston$crim[folds==k]-pred)^2) # find MSE of each fold
  }
} # write a double for loop to fit, predict, and calculate cross-validation errors

mse.cv = apply(cv.errors, 2, mean)                # find test error using Mean Squared Error
mse.cv
min(mse.cv)                                       # find minimum test error using Mean Squared Error


rmse.cv = sqrt(apply(cv.errors,2,mean))           # find RMSE
rmse.cv
min(rmse.cv)                                      # find minimum RMSE
plot(rmse.cv, pch=19, xlab = "Number of Features", ylab="RMSE", type="b") # plot

```


The Lasso Model
---------------

```{r}

#### LASSO



library(glmnet) # load package
x.train = model.matrix(crim~., data=training.set) # set up predictors from the training set
y.train = training.set$crim                       # set up response from the training set

x.test = model.matrix(crim~., data=test.set)      # set up predictors from the test set
y.test = test.set$crim                            # set up response from the test set

fit.lasso = glmnet(x.train, y.train, alpha=1)     # fit lasso model

cv.lasso = cv.glmnet(x.train, y.train, alpha=1)   # generate  位 by cross-validation, with k=10 by default
cv.lasso
bestlambda = cv.lasso$lambda.min                  # choose minimum lambda model

pred.lasso = predict(fit.lasso, s=bestlambda, newx=x.test)  # make prediction on the test set 
error.lasso = mean((y.test - pred.lasso)^2)       # find test error using Mean Squared Error
error.lasso
rmse.lasso = sqrt(error.lasso)                    # find RMSE of lasso model
rmse.lasso

coef(fit.lasso, s=bestlambda)                     # show the non-zero coefficient estimates

```


Ridge Regression Model
----------------------

```{r}
#### RIGDE
# model selection using ridge regression


fit.ridge = glmnet(x.train, y.train, alpha=0)     # fit ridge regression

cv.ridge = cv.glmnet(x.train, y.train, alpha=0)   # generate  位 by cross-validation, with k=10 by default
cv.ridge
bestlambda = cv.ridge$lambda.min                  # choose minimum lambda model

pred.ridge = predict(fit.ridge, s=bestlambda, newx=x.test) # make prediction on the test set
error.ridge = mean((y.test - pred.ridge)^2)       # find test error using Mean Squared Error
error.ridge
rmse.ridge = sqrt(error.ridge)                    # find RMSE of ridge regression model
rmse.ridge

coef(fit.ridge, s=bestlambda)                  	  # show the non-zero coefficient estimates

```



(b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, cross-validation, or some other reasonable alternative, as opposed to using training error.

Answer:
======

(c) Does your chosen model involve all of the features in the data set? Why or why not?

Answer:
======
